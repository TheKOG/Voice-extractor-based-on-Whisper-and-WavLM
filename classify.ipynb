{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    device='cuda'\n",
    "    RATE=16000\n",
    "    save_folder='slices'\n",
    "    thresold1=0.5\n",
    "    thresold=0.5\n",
    "    batch_size=8\n",
    "    epoches=1000\n",
    "    fuck=999\n",
    "    lr=3e-5\n",
    "    strides=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas=[]\n",
    "with open('list.txt','r',encoding='utf-8') as f:\n",
    "    rows=f.readlines()\n",
    "    for i,row in enumerate(rows):\n",
    "        if(row==''):\n",
    "            break\n",
    "        id,setence=row.split(':')\n",
    "        # print(id)\n",
    "        id=int(id)\n",
    "        datas.append({'id':id,'sentence':setence,'wav':CFG.save_folder+'/{}.wav'.format(id)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Wav(filename = \"./slices/1.wav\"):\n",
    "    waveform,sample_rate = torchaudio.load(filename)\n",
    "    if(len(waveform[0])<10):\n",
    "        return waveform[0].cpu().numpy()\n",
    "    # print(\"Shape of waveform:{}\".format(waveform.size())) #音频大小\n",
    "    # print(\"sample rate of waveform:{}\".format(sample_rate))#采样率\n",
    "    resampler = torchaudio.transforms.Resample(sample_rate, CFG.RATE)\n",
    "    if(sample_rate!=CFG.RATE):\n",
    "        waveform=resampler(waveform)\n",
    "    return waveform[0].cpu().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WaveLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, WavLMForXVector\n",
    "pretrained = WavLMForXVector.from_pretrained(\"microsoft/wavlm-base-plus-sv\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"microsoft/wavlm-base-plus-sv\")\n",
    "pretrained=pretrained.to(CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "\n",
    "tmp=[]\n",
    "for i,data in enumerate(datas):\n",
    "    try:\n",
    "        tmp.append(Load_Wav(data['wav']))\n",
    "    except:\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "x=feature_extractor(tmp, return_tensors=\"pt\",sampling_rate=CFG.RATE, padding=True).to(CFG.device)\n",
    "# print(x)\n",
    "# 'input_values','attention_mask'\n",
    "# exit()\n",
    "\n",
    "\n",
    "n=len(x['input_values'])\n",
    "strides=int(math.ceil(n/CFG.batch_size))\n",
    "embeddings=[]\n",
    "with torch.no_grad():\n",
    "    for stride in tqdm(range(strides)):\n",
    "        st=stride*CFG.batch_size\n",
    "        ed=min(st+CFG.batch_size,n)\n",
    "        x_={'input_values':x['input_values'][st:ed],'attention_mask':x['attention_mask'][st:ed]}\n",
    "        embeddings.append(pretrained(**x_).embeddings.detach())\n",
    "\n",
    "embeddings=torch.cat(embeddings,dim=0)\n",
    "with open('embeddings.pkl','wb') as f:\n",
    "    pickle.dump(embeddings,f,-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        self.fc1=nn.Linear(512,32)\n",
    "        self.fc2=nn.Linear(32,2)\n",
    "        self.softmax=nn.Softmax()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=x.to(CFG.device)\n",
    "        x=self.fc1(x)\n",
    "        x=self.fc2(x)\n",
    "        x=self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model=Model().to(CFG.device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.8.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import tkinter as tk\n",
    "import pygame\n",
    "import _thread\n",
    "import time\n",
    "\n",
    "audio_file = \"tmp.wav\"\n",
    "cls=-1\n",
    "\n",
    "def play_audio():\n",
    "    try:\n",
    "        pygame.mixer.init()\n",
    "        audio = pygame.mixer.Sound(audio_file)\n",
    "        _thread.start_new_thread(audio.play,())\n",
    "    except:\n",
    "        pass\n",
    "    # audio.play()\n",
    "\n",
    "def classify_as_1():\n",
    "    global cls\n",
    "    cls=1\n",
    "\n",
    "def classify_as_0():\n",
    "    global cls\n",
    "    cls=0\n",
    "\n",
    "is_running=True\n",
    "\n",
    "def on_closing(root):\n",
    "    global is_running  # 标记主线程结束\n",
    "    is_running = False\n",
    "    root.destroy()\n",
    "\n",
    "def init():\n",
    "    global play_button\n",
    "    # 创建 GUI 窗口\n",
    "    root = tk.Tk()\n",
    "    # 设置窗口标题和尺寸\n",
    "    root.title(\"Wav Classification\")\n",
    "    root.wm_attributes(\"-topmost\", True)\n",
    "    root.geometry(\"400x200\")\n",
    "    play_button = tk.Button(root, text=\"Wait a moment...\", command=play_audio)\n",
    "    play_button.pack()\n",
    "\n",
    "    # 创建两个按钮来决定音频的分类\n",
    "    classify_1_button = tk.Button(root, text=\"Positive\", command=classify_as_1)\n",
    "    classify_1_button.pack(side=tk.LEFT, padx=50)\n",
    "\n",
    "    classify_2_button = tk.Button(root, text=\"Negative\", command=classify_as_0)\n",
    "    classify_2_button.pack(side=tk.RIGHT, padx=50)\n",
    "    root.protocol(\"WM_DELETE_WINDOW\", lambda:on_closing(root))\n",
    "    return root\n",
    "\n",
    "\n",
    "def UI():\n",
    "    root=init()\n",
    "    root.mainloop()\n",
    "\n",
    "def Test(wavepth):\n",
    "    time.sleep(0.2)\n",
    "    global cls\n",
    "    global audio_file\n",
    "    cls=-1\n",
    "    audio_file=wavepth\n",
    "    play_audio()\n",
    "    while is_running:\n",
    "        play_button.config(text='Play '+audio_file)\n",
    "        # print(cls,end=' ')\n",
    "        if(cls!=-1):\n",
    "            if(cls==0):\n",
    "                return torch.as_tensor([1.,0.])\n",
    "            else:\n",
    "                return torch.as_tensor([0.,1.])\n",
    "        time.sleep(0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1201, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hasee\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from transformers import AdamW\n",
    "import math\n",
    "import os\n",
    "\n",
    "f=open('embeddings.pkl','rb')\n",
    "embeddings=pickle.load(f).to(CFG.device)\n",
    "print(embeddings.shape)\n",
    "\n",
    "optimizer=AdamW(model.parameters(),lr=CFG.lr)\n",
    "\n",
    "train=[]\n",
    "\n",
    "def Train():\n",
    "    for epoch in tqdm(range(CFG.epoches)):\n",
    "        tmp_x=[]\n",
    "        tmp_label=[]\n",
    "        for data in train:\n",
    "            embed,label=data\n",
    "            tmp_x.append(embed)\n",
    "            tmp_label.append(label)\n",
    "        tmp_x=torch.stack(tmp_x)\n",
    "        tmp_label=torch.stack(tmp_label).to(CFG.device)\n",
    "        out=model(tmp_x)\n",
    "        loss=((out-tmp_label)**2).mean()\n",
    "        if(epoch%CFG.fuck==0):\n",
    "            print(\"stride={0} epoch={1} loss={2}\".format(stride,epoch,loss.item()))\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]c:\\Users\\Hasee\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\wavlm\\modeling_wavlm.py:1755: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (input_length - kernel_size) // stride + 1\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.33it/s]\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]C:\\Users\\Hasee\\AppData\\Local\\Temp\\ipykernel_8800\\929340445.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x=self.softmax(x)\n",
      "  7%|▋         | 67/1000 [00:00<00:01, 669.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stride=0 epoch=0 loss=0.2790892720222473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 810.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stride=0 epoch=999 loss=0.06445500999689102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folder0 = r'./samples/0'\n",
    "folder1 = r'./samples/1'\n",
    "\n",
    "tmp0=[]\n",
    "tmp1=[]\n",
    "for root, dirs, files in os.walk(folder0):\n",
    "    # print\n",
    "    for file in files:\n",
    "        # print('fuckpps')\n",
    "        path = os.path.join(root, file)\n",
    "        wav=Load_Wav(path)\n",
    "        tmp0.append(wav)\n",
    "\n",
    "for root, dirs, files in os.walk(folder1):\n",
    "    # print\n",
    "    for file in files:\n",
    "        # print('fuckpps')\n",
    "        path = os.path.join(root, file)\n",
    "        wav=Load_Wav(path)\n",
    "        tmp1.append(wav)\n",
    "\n",
    "x0=feature_extractor(tmp0, return_tensors=\"pt\",sampling_rate=CFG.RATE, padding=True).to(CFG.device)\n",
    "x1=feature_extractor(tmp1, return_tensors=\"pt\",sampling_rate=CFG.RATE, padding=True).to(CFG.device)\n",
    "\n",
    "n0=len(x0['input_values'])\n",
    "strides0=int(math.ceil(n0/CFG.batch_size))\n",
    "embeddings0=[]\n",
    "with torch.no_grad():\n",
    "    for stride in tqdm(range(strides0)):\n",
    "        st=stride*CFG.batch_size\n",
    "        ed=min(st+CFG.batch_size,n0)\n",
    "        x_={'input_values':x0['input_values'][st:ed],'attention_mask':x0['attention_mask'][st:ed]}\n",
    "        embeddings0.append(pretrained(**x_).embeddings.detach())\n",
    "embeddings0=torch.cat(embeddings0,dim=0)\n",
    "\n",
    "n1=len(x1['input_values'])\n",
    "strides1=int(math.ceil(n1/CFG.batch_size))\n",
    "embeddings1=[]\n",
    "with torch.no_grad():\n",
    "    for stride in tqdm(range(strides1)):\n",
    "        st=stride*CFG.batch_size\n",
    "        ed=min(st+CFG.batch_size,n1)\n",
    "        x_={'input_values':x1['input_values'][st:ed],'attention_mask':x1['attention_mask'][st:ed]}\n",
    "        embeddings1.append(pretrained(**x_).embeddings.detach())\n",
    "embeddings1=torch.cat(embeddings1,dim=0)\n",
    "        \n",
    "label=torch.as_tensor([1.,0.])\n",
    "for embed in embeddings0:\n",
    "    train.append([embed,label])\n",
    "label=1-label\n",
    "for embed in embeddings1:\n",
    "    train.append([embed,label])\n",
    "Train()\n",
    "# train.clear()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hasee\\AppData\\Local\\Temp\\ipykernel_8800\\929340445.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x=self.softmax(x)\n"
     ]
    }
   ],
   "source": [
    "_thread.start_new_thread(UI,())\n",
    "apr=[]\n",
    "for stride in range(CFG.strides):\n",
    "    with torch.no_grad():\n",
    "        out=model(embeddings)\n",
    "        sum=0\n",
    "        for i,p in enumerate(out):\n",
    "            if(p[1]<CFG.thresold1):\n",
    "                continue\n",
    "            if(i in apr):\n",
    "                continue\n",
    "            label=Test(datas[i]['wav'])\n",
    "            apr.append(i)\n",
    "            # print('fuckpps')\n",
    "            train.append([embeddings[i],label])\n",
    "            if(label[1]>=1.):\n",
    "                sum+=1\n",
    "            if sum>=10:\n",
    "                break\n",
    "    Train()\n",
    "    # train.clear()\n",
    "is_running=False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hasee\\AppData\\Local\\Temp\\ipykernel_6724\\929340445.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x=self.softmax(x)\n"
     ]
    }
   ],
   "source": [
    "out=model(embeddings)\n",
    "result=[]\n",
    "for j,p in enumerate(out):\n",
    "    if(p[1]<CFG.thresold):\n",
    "        continue\n",
    "    result.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "print(len(result))\n",
    "with open('result.pkl','wb') as f:\n",
    "    pickle.dump(result,f,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def mkdir(pth):\n",
    "    if(not os.path.exists(pth)):\n",
    "        os.mkdir(pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import shutil\n",
    "f=open('result.pkl','rb')\n",
    "result=pickle.load(f)\n",
    "mkdir('result')\n",
    "mkdir('result/audios')\n",
    "# shutil.copy('list.txt','result/list_.txt')\n",
    "f_list=open('result/result.txt','w',encoding='utf-8')\n",
    "for i,id in enumerate(result):\n",
    "    ele=datas[id]\n",
    "    wav=ele['wav']\n",
    "    text=ele['sentence']\n",
    "    shutil.copy(wav,'result/audios/{0}.wav'.format(i))\n",
    "    f_list.write(str(i)+':'+text)\n",
    "f_list.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7rc1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3791a47348c2704d85b6abcab0f16aa85fb80860f783c67628ddd43e9bf228ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
